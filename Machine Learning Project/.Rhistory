mu<-mean(galton$child)
sTot<-1+1
sTot <- sum((galton$child-mu)^2)
sRes<-sum(2)
sRes <- deviance(fit)
sRes/sTot
1-sRes/sTot
summary(fit)$r
summary(fit)$r.squared
cor(galton$child, galton$parent)
cor(galton$parent,galton$child)^2
ones <- rep(1, nrow(galton))
lm(child ~ ones + parent -1, galton)
lm(child ~ parent, galton)
lm(child ~ 1, galton)
head(trees)
fit<-lm(Volume ~ Girth + Height + Constant -1, trees)
trees2 <- eliminate("Girth", trees)
head(trees2)
fit2<-lm(Volume ~ Height + Constant -1,trees2)
lapply(list(fit, fit2), coef)
all<-lm(Fertility~Agriculture+Examination+Education+Catholic+Infant.Mortality, swiss)
summary(all)
lm(FertilityÃ griculture, swiss)
lm(Fertility~griculture, swiss)
lm(Fertility~agriculture, swiss)
lm(Fertility ~ Agriculture, swiss)
summary(lm(Fertility ~ Agriculture, swiss))
cor(swiss$Examination, swiss$Education)
cor(swiss$Agriculture, swiss$Education)
makelms()
ec<-sum(swiss$Examination)
ec<-swiss$Examination+swiss$Catholic
ec
efit <- lm(Fertility ~ . + ec, swiss)
efit <- lm(Fertility ~ . + ec, swiss)
all$coefficients-efit$coefficients
x <- c(0.61, 0.93, 0.83, 0.35, 0.54, 0.16, 0.91, 0.62, 0.62)
y <- c(0.67, 0.84, 0.6, 0.18, 0.85, 0.47, 1.1, 0.65, 0.36)
reg<-lm(y~x)
summary(reg)
reg$sigma
summary(reg)$sigma
data(mtcars)
head(mtcars)
reg<-lm(mpg~wt, mtcars)
summary(reg)
predict(reg,data.frame(x=mean(mtcars$wt), interval=("confidence")))
predict(reg,newdata = data.frame(x=mean(mtcars$wt), interval=("confidence")))
predict(reg,newdata = data.frame(x=mean(mtcars$wt)), interval=("confidence"))
y <- mtcars$mpg
x <- mtcars$wt
fit_car <- lm(y ~ x)
predict(fit_car, newdata = data.frame(x = mean(x)), interval = ("confidence"))
yhat <- fit_car$coef[1] + fit_car$coef[2] * mean(x)
yhat + c(-1, 1) * qt(.975, df = fit_car$df) * summary(fit_car)$sigma / sqrt(length(y))
predict(fit_car, newdata = data.frame(x = 3), interval = ("prediction"))
yhat <- fit_car$coef[1] + fit_car$coef[2] * 3
yhat + c(-1, 1) * qt(.975, df = fit_car$df) * summary(fit_car)$sigma * sqrt(1 + (1/length(y)) + ((3 - mean(x)) ^ 2 / sum((x - mean(x)) ^ 2)))
fit3 <- lm(mpg~I(wt*0.5),data=mtcars)
summary(fit3)
confint(fit3)
fit4 <- lm(mpg~wt,data=mtcars)
fit5 <- lm(mpg~1,data=mtcars)
num <- sum((predict(fit4)-mtcars$mpg)^2)
den <- sum((predict(fit5)-mtcars$mpg)^2)
num/den
swirl()
install.packages("swirl")
install.packages("swirl")
library(swirl)
install_from_swirl("Regression Models")
swirl()
6
dim(InsectSprays)
head(InsectSprays, 15)
A
sx$A
sx
sA
summary(m[,2])
summary(M[,2])
summary(B[,2])
summary(InsectSprays[,2])
sapply(InsectSprays, class)
lm(count ~ spray, InsectSprays)
fit<-lm(count ~ spray, InsectSprays)
summary(fit)$coef
est<-summary(fit)$coef[1,]
est <- summary(fit)$coef[,1]
mean(InsectSprays$spray)
mean(InsectSprays$count[spray=="A",])
mean(InsectSprays$count[,spray=="A"])
mean(sA)
mean(sB)
nfit<-lm(count ~ spray -1)
nfit<-lm(count ~ spray -1, InsectSprays)
summary(nfit)$coef
spray2<-relevel(InsectSprays$spray, "C")
fit2<-lm(count ~ spray2, InsectSprays)
summary(fit2)$coef
mean(sC)
(fit$coef[3])
(fit$coef[2]-fit$coef[3])/1.6011
dim(hunger)
948
names(hunger)
lm(Numeric ~ predictor, hunger)
lm(Numeric ~ Year, hunger)
fit <- lm(Numeric ~ Year, hunger)
summary(fit)$coef
lmF<-lm(Numeric ~ hunger[hunger$Sex=="Female"], hunger)
lmF<-lm(Numeric ~ Year, hunger)
lmF <- lm(hunger$Numeric[hunger$Sex=="Female"] ~ hunger$Year[hunger$Sex=="Female"])
lmF <- lm(hunger$Numeric[hunger$Sex=="male"] ~ hunger$Year[hunger$Sex=="male"])
lmM <- lm(hunger$Numeric[hunger$Sex=="male"] ~ hunger$Year[hunger$Sex=="male"])
summary(hunger$Sex)
lmM <- lm(hunger$Numeric[hunger$Sex=="Male"] ~ hunger$Year[hunger$Sex=="Male"])
lmBoth<-lm(Numeric ~ Year + Sex, hunger)
summary(lmBoth)$coef
summary(lmBoth)
lmInter<-lm(Numeric~Year + Sex + Sex*Year, hunger)
summary(lmInter)
fit<-(y~x, out2)
fit<-lm(y~x, out2)
plot(fit, which = 1)
fitno<-lm(y~x[-1], out2)
fitno<-lm(y~x[-1,], out2)
fitno<-lm(y~x, out2)
fitno <- lm(y ~ x, out2[-1, ])
plot(fitno, which = 1)
coef(fit)-coef(fitno)
head(dfbeta(fit))
resno <- out2[1, "y"] - predict(fitno, out2[1,])
1-resid(fit)[1]/resno
head(hatvalues(fit))
2
sigma <- sqrt(deviance(fit)/df.residual(fit))
s
3
rstd <- resid(fit)/(sigma * sqrt(1-hatvalues(fit)))
head(cbind(rstd, rstandard(fit)))
plot(fit, which=3)
plot(fit, which=2)
2
sigma1 <- sqrt(deviance(fitno)/df.residual(fitno))
2
resid(fit)[1]/(sigma1*sqrt(1-hatvalues(fit)[1]))
3
head(rstudent(fit))
predict(fitno, out2)-predict(fit, out2)
dy <- predict(fitno, out2)-predict(fit, out2)
3
sum(dy^2)/(2*sigma^2)
plot(fit, which=5)
data(mtcars)
str(mtcars)
regressin<-lm(mpg~ factor(cyl)+wt, mtcars)
summary(regressin)
reg2<-lm(mpg ~ factor(cyl), mtcars)
summary(rreformulate())
summary(reg2)
fit_interaction <- lm(mpg ~ cyl + wt + cyl:wt, mtcars)
summary(fit_interaction)
anova(regressin, fit_interaction, test="Chisq")
fit_interaction <- lm(mpg ~ factor(cyl)*wt, mtcars)
anova(regressin, fit_interaction, test="Chisq")
lm(mpg ~ I(wt * 0.5) + factor(cyl), data = mtcars)
x <- c(0.586, 0.166, -0.042, -0.614, 11.72)
y <- c(0.549, -0.026, -0.127, -0.751, 1.344)
regre<- lm(y~x)
hatvalues(regre)
x <- c(0.586, 0.166, -0.042, -0.614, 11.72)
y <- c(0.549, -0.026, -0.127, -0.751, 1.344)
regre<-lm(y~x)
dfbeta(regre)
dfbeta(regre)$slope
dfbetas(regre)[, 2]
dfbeta(regre)
swirl()
rgp1()
rgp2()
head(swiss)
mdl<-lm(Fertility~., swiss)
vif(mdl)
mdl2<-lm(Fertility ~ Agriculture + Education + Catholic + Infant.Mortality, swiss )
vif(mdl2)
x1c<-simbias()
apply(x1c, 1, mean)
fit1<-lm(Fertility ~Agriculture, swiss)
fit3<-lm(Fertility ~ Agriculture + Examination + Education, swiss)
anova(fit1, fit3)
deviance(fit3)
dz<- deviance(fit3)/43
d<- deviance(fit3)/43
n<-deviance(fit1)-deviance(fit3)
n <- (deviance(fit1) - deviance(fit3))/2
n/d
pf(n/d, 2, 43, lower.tail=FALSE)
shapiro.test(fit3$residuals)
anova(fit1, fit3, fit5, fit6)
ravenData
msl<-glm(ravenWinNum ~ ravenScore, family = "binomial", ravenData)
mdl <- glm(ravenWinNum ~ ravenScore, binomial, ravenData)
lodds<-predict(mdl, data.frame(ravenScore=c(0, 3, 6)))
exp(lodds)/(1+exp(lodds))
summary(mdl)
2
exp(confint(mdl))
anova(mdl)
qchisq(0.95, 1)
var(rpois(1000, 50))
nxt()
View(hits)
class(hits[,'date'])
as.integer(head(hits[,"date"]))
mdl <- glm(visits ~ date, poisson, hits)
summary(mdl)
confint(mdl, "date")
exp(confint(mdl, 'date'))
which.max(hits[,'visits'])
hits[704,]
mdl$fitted.values[704]
lambda <- mdl$fitted.values[704]
qpois(.95, lambda)
2
mdl2 <- glm(formula = simplystats ~ date, family = poisson, data = hits, offset = log(visits + 1))
qpois(.95, mdl2$fitted.values[704])
library(MASS)
data("shuttle")
summary(shuttle)
2.6741/2.7300
35.274443/37.031917
0.07580980/0.07372098
2.6741486/2.7300291
1.448048e-272/3.510670e-300
data("mtcars")
summary(mtcars)
str(mtcars)
str(mtcars)
?step
?anova
knitr::opts_chunk$set(echo = TRUE)
data("mtcars")
str(mtcars)
boxplot(mpg~am,col=c("blue","green"),main="Boxplot of MPG vs Transmission",xlab="Transmission (0=Automatic,1=Manual)",ylab="MPG")
boxplot(mpg~am,col=c("blue","green"),main="Boxplot of MPG vs Transmission",xlab="Transmission (0=Automatic,1=Manual)",ylab="MPG")
boxplot(mpg~am,col=c("blue","green"),main="Boxplot of MPG vs Transmission",xlab="Transmission (0=Automatic,1=Manual)",ylab="MPG", data = mtcars)
mtcars$drat <- factor(mtcars$drat)
mtcars$cyl <- factor(mtcars$cyl)
mtcars$vs <- factor(mtcars$vs)
mtcars$gear <- factor(mtcars$gear)
mtcars$carb <- factor(mtcars$carb)
mtcars$am <- factor(mtcars$am,labels=c('Automatic','Manual'))
boxplot(mpg~am,col=c("blue","green"),data= mtcars, main="Boxplot of MPG vs Transmission",xlab="Transmission (0=Automatic,1=Manual)",ylab="MPG")
knitr::opts_chunk$set(echo = TRUE)
data("mtcars")
str(mtcars)
mtcars$drat <- factor(mtcars$drat)
mtcars$cyl <- factor(mtcars$cyl)
mtcars$vs <- factor(mtcars$vs)
mtcars$gear <- factor(mtcars$gear)
mtcars$carb <- factor(mtcars$carb)
mtcars$am <- factor(mtcars$am,labels=c('Automatic','Manual'))
boxplot(mpg~am,col=c("blue","green"),data= mtcars, main="Boxplot of MPG vs Transmission",xlab="Transmission (0=Automatic,1=Manual)",ylab="MPG")
boxplot(mpg~am,col=am, data= mtcars, main="Boxplot of MPG vs Transmission",xlab="Transmission (0=Automatic,1=Manual)",ylab="MPG")
boxplot(mpg~am,col="am", data= mtcars, main="Boxplot of MPG vs Transmission",xlab="Transmission (0=Automatic,1=Manual)",ylab="MPG")
boxplot(mpg~am, col=mtcars$am, data= mtcars, main="MPG vs Transmission",xlab="Transmission (Automatic, Manual)",ylab="MPG")
boxplot(mpg~am, data= mtcars, main="MPG vs Transmission",xlab="Transmission",ylab="MPG")
par(mfrow=c(2,2))
plot(best_model,col="blue")
simple_model <- lm(mpg~am,data=mtcars) # Simple Model
summary(simple_model)
base_model <- lm(mpg~.,data=mtcars) # Including all variables
summary(base_model)
best_model <- step(base_model,direction="both") # Selecting the best model
summary(best_model)
par(mfrow=c(2,2))
plot(best_model,col="blue")
library(ggplot2)
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
library(AppliedPredictiveModeling)
library(caret)
library(AppliedPredictiveModeling)
install.packages("caret")
install.packages("caret")
library(AppliedPredictiveModeling)
install.packages("AppliedPredictiveModeling")
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
names(training
)
featurePlot(x = training[, 1:8], y = training$CompressiveStrength, plot = "pairs")
library(hmisc)
install.packages("Hmisc"")
""
install.packages("Hmisc")
str(concrete)
library(Hmisc)
index <- seq_along(1:nrow(training))
ggplot(data = training, aes(x = index, y = CompressiveStrength, color=index)) + geom_point() +
theme_bw()
index <- seq_along(1:nrow(training))
ggplot(data = training, aes(x = index, y = CompressiveStrength)) + geom_point() +
theme_bw()
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[inTrain, ]
testing = mixtures[-inTrain, ]
index <- seq_along(1:nrow(training))
ggplot(data = training, aes(x = index, y = CompressiveStrength)) + geom_point() +
theme_bw()
index <- seq_along(1:nrow(training))
ggplot(data = training, aes(x = index, y = CompressiveStrength, col = colnames(training) )) + geom_point() +
theme_bw()
index <- seq_along(1:nrow(training))
ggplot(data = training, aes(x = index, y = CompressiveStrength, col = colnames(training)) + geom_point() +
theme_bw()
)
index <- seq_along(1:nrow(training))
ggplot(data = training, aes(x = index, y = CompressiveStrength, col = colnames(training))) + geom_point() +
theme_bw()
index <- seq_along(1:nrow(training))
ggplot(data = training, aes(x = index, y = CompressiveStrength, col = colnames())) + geom_point() +
theme_bw()
index <- seq_along(1:nrow(training))
ggplot(data = training, aes(x = index, y = CompressiveStrength, col = colnames(training))) + geom_point() +
theme_bw()
index <- seq_along(1:nrow(training))
ggplot(data = training, aes(x = index, y = CompressiveStrength, color = colnames(training))) + geom_point() +
theme_bw()
index <- seq_along(1:nrow(training))
ggplot(data = training, aes(x = index, y = CompressiveStrength, color = names)) + geom_point() +
theme_bw()
index <- seq_along(1:nrow(training))
ggplot(data = training, aes(x = index, y = CompressiveStrength, col = names)) + geom_point() +
theme_bw()
names<-colnames(training)
index <- seq_along(1:nrow(training))
ggplot(data = training, aes(x = index, y = CompressiveStrength, col = names)) + geom_point() +
theme_bw()
index <- seq_along(1:nrow(training))
ggplot(data = training, aes(x = index, y = CompressiveStrength, color = names)) + geom_point() +
theme_bw()
index <- seq_along(1:nrow(training))
ggplot(data = training, aes(x = index, y = CompressiveStrength)) + geom_point() +
theme_bw()
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]training = adData[ inTrain,]
testing = adData[-inTrain,]
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433); data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]training = adData[ inTrain,]
testing = adData[-inTrain,]
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433); data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]] ;training = adData[ inTrain,]
testing = adData[-inTrain,]
cor<-cor(AlzheimerDisease)
AlzheimerDisease
names(training)
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
library(pgmm)
data(olive)
olive = olive[,-1]
install.packages("pgmm")
library(pgmm)
data(olive)
olive = olive[,-1]
names(olive)
modelfit<- train(Area~.,method="rpart", olive)
newdata = as.data.frame(t(colMeans(olive)))
prediction<-predict(modelfit,newdata)
prediction
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
install.packages("ElemStatLearn")
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
rm(list=ls())
rm(list=ls())
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
?ts
getwd()
setwd("C:\Users\User\Desktop\Data Analysis Especialization\8. Machine LEarning\Project")
setwd("C:/Users/User/Desktop/Data Analysis Especialization/8. Machine LEarning/Project")
set.seed(1234)
trainingset<-read.csv("pml-training.csv", na.strings = c("NA","#DIV/0!",""))
testingset<- read.csv("pml-testing.csv", na.strings = c("NA","#DIV/0!",""))
is.na(trainingset)
colSums(is.na(trainingset)) == 0
trainingset[,colSums(is.na(trainingset)) == 0]
?complete.cases
dim(trainingset)
dim(testingset)
summary(trainingset);
colnames(trainingset)
library(ggplot2); library(caret);library(randomForest);library(rpart.plot)
library(rpart.plot)
library(caret)
trainingset<-trainingset[,colSums(is.na(trainingset)) == 0]
testingset <-testingset[,colSums(is.na(testingset)) == 0]
trainingset   <-trainingset[,-c(1:7)]
testingset <-testingset[,-c(1:7)]
sub_trainingset <- createDataPartition(y=trainingset$classe, p=0.75, list=FALSE)
TrTrainingSet <- trainingset[sub_trainingset, ]
TeTrainingSet <- trainingset[-sub_trainingset, ]
qplot(classe, TrTrainingSet)
qplot(classe, data = TrTrainingSet)
qplot(classe, data = TrTrainingSet, main = "Classe Variables Histogram", xlab="classe", ylab = "Frequency")
qplot(classe, data = TrTrainingSet, main = "Classe Variables Histogram", xlab="classe", ylab = "Frequency", position = "center")
qplot(classe, data = TrTrainingSet, main = "Classe Variables Histogram", xlab="classe", ylab = "Frequency")
qplot(classe, data = TrTrainingSet, main = "Classe Variables Histogram", xlab="classe", ylab = "Frequency", col = "blue")
qplot(classe, data = TrTrainingSet, main = "Classe Variables Histogram", xlab="classe", ylab = "Frequency", colour = "blue")
qplot(classe, data = TrTrainingSet, main = "Classe Variables Histogram", xlab="classe", ylab = "Frequency", col = "yellow")
qplot(classe, data = TrTrainingSet, main = "Classe Variables Histogram", xlab="classe", ylab = "Frequency")
fitmodel1<- train(classe ~ ., data = TrTrainingSet, method = "rpart")
install.packages("rpart")
install.packages("rpart")
fitmodel1<- train(classe ~ ., data = TrTrainingSet, method = "rpart")
install.packages('e1071')
set.seed(1234)
fitmodel1<- train(classe ~ ., data = TrTrainingSet, method = "rpart")
library(rattle)
install.packages("rattle")
library(rattle)
fancyRpartPlot(fitmodel1$finalModel)
fancyRpartPlot(fitmodel1$finalModel, type = 1)
fancyRpartPlot(fitmodel1$finalModel, type = 3)
fancyRpartPlot(fitmodel1$finalModel, type = 4)
fancyRpartPlot(fitmodel1$finalModel, type = 5)
fancyRpartPlot(fitmodel1$finalModel, type = 6)
fancyRpartPlot(fitmodel1$finalModel, type = 4)
rpart.plot(fitmodel1, extra=102, under=TRUE, faclen=0)
rpart.plot(fitmodel1$finalModel, extra=102, under=TRUE, faclen=0)
fancyRpartPlot(fitmodel1$finalModel, type = 4)
rpart.plot(fitmodel1, extra=102, under=TRUE, faclen=0)
prediction1 <- predict(fitmodel1, TeTrainingSet, type = "class")
prediction1 <- predict(fitmodel1$finalModel, TeTrainingSet, type = "class")
confusionMatrix(prediction1, TeTrainingSet$classe)
summary(prediction1$Classe)
summary(TeTrainingSet$Classe)
summary(TeTrainingSet)
str(TeTrainingSet)
prediction1
TeTrainingSet <- trainingset[-sub_trainingset, ]
confusionMatrix(prediction1, TeTrainingSet$classe)
TeTrainingSet$classe <- as.factor(TeTrainingSet$classe)
str(TeTrainingSet)
confusionMatrix(prediction1, TeTrainingSet$classe)
sub_trainingset <- createDataPartition(y=trainingset$classe, p=0.75, list=FALSE)
TrTrainingSet <- trainingset[sub_trainingset, ]
TrTrainingSet$classe <- as.factor(TrTrainingSet$classe)
TeTrainingSet <- trainingset[-sub_trainingset, ]
TeTrainingSet$classe <- as.factor(TeTrainingSet$classe)
fitmodel1<- train(classe ~ ., data = TrTrainingSet, method = "rpart")
fitmodel1<- train(classe ~ ., data = TrTrainingSet, method = "rpart")
prediction1 <- predict(fitmodel1$finalModel, TeTrainingSet, type = "class")
confusionMatrix(prediction1, TeTrainingSet$classe)
install.packages("rpart")
install.packages("rpart")
fitmodel1 <- rpart(classe ~ ., data=TrainTrainingSet, method="class")
fitmodel1 <- rpart(classe ~ ., data=TrTrainingSet, method="class")
prediction1 <- predict(fitmodel1, TeTrainingSet, type = "class")
confusionMatrix(prediction1, TeTrainingSet$classe)
fitmodel2<-train(classe~., data=TrTrainingSet, method ="rf")
fitmodel2<-train(classe~., data=TrTrainingSet, method ="rf")
